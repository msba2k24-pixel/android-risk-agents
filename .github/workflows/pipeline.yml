name: Android Risk Agents Pipeline

on:
  workflow_dispatch:
  schedule:
    - cron: "0 14 * * *"  # daily at 14:00 UTC

concurrency:
  group: android-risk-agents-pipeline
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 25

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ---- Your existing steps here (seed sources, scrape, embed, etc.) ----
      # - name: Seed sources (idempotent)
      #   run: python -m src.seed_sources
      # - name: Scrape sources
      #   run: python -m src.scrape_sources

      - name: Warm up Modal vLLM (OpenAI-compatible)
        env:
          LLM_BASE_URL: ${{ secrets.LLM_BASE_URL }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
        run: |
          python - << 'PY'
          import os, sys
          import requests

          base = os.environ["LLM_BASE_URL"].rstrip("/")
          key = os.environ["LLM_API_KEY"]

          # Try the lightest endpoint first.
          # Many vLLM deployments expose /v1/models.
          url = f"{base}/v1/models"
          headers = {"Authorization": f"Bearer {key}"}

          try:
            r = requests.get(url, headers=headers, timeout=20)
            print("Warmup /v1/models:", r.status_code)
            # If /v1/models isn't available, fallback to a tiny chat completion.
            if r.status_code >= 400:
              raise RuntimeError("models endpoint not OK")
          except Exception:
            url2 = f"{base}/v1/chat/completions"
            payload = {
              "model": os.getenv("MODEL_TRIAGE", "mistral-small"),
              "messages": [{"role": "user", "content": "warmup"}],
              "max_tokens": 5,
              "temperature": 0
            }
            r2 = requests.post(url2, headers=headers, json=payload, timeout=30)
            print("Warmup /v1/chat/completions:", r2.status_code)
            if r2.status_code >= 400:
              print(r2.text[:300])
              sys.exit(1)
          PY

      - name: Generate insights (Modal)
        env:
          # Supabase secrets you already have
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

          # Modal vLLM OpenAI-compatible endpoint
          LLM_BASE_URL: ${{ secrets.LLM_BASE_URL }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

          # Model selection
          MODEL_TRIAGE: ${{ secrets.MODEL_TRIAGE }}
          MODEL_ANALYZE: ${{ secrets.MODEL_ANALYZE }}

          # Optional knobs
          RELEVANCE_THRESHOLD: "70"
          AGENT_NAME: "modal-digital-risk-agent"
        run: |
          python -m src.generate_insights_monstral
