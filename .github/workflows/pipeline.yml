name: Android Risk Agents Pipeline

on:
  workflow_dispatch:

concurrency:
  group: android-risk-agents-pipeline
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache HF models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: hf-${{ runner.os }}-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install -r requirements.txt

      - name: Seed sources (idempotent)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          USER_AGENT: "android-risk-agents-bot/0.1 (github-actions)"
        run: python -m src.seed_sources

      - name: Discover latest bulletins
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          USER_AGENT: "android-risk-agents-bot/0.1 (github-actions)"
        run: python -m src.discover_bulletins

      - name: Scrape sources (embeddings created here)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          USER_AGENT: "android-risk-agents-bot/0.1 (github-actions)"
          VECTOR_DIM: "384"
          EMBED_BASELINE_ON_FIRST_SNAPSHOT: "true"
        run: python -m src.scrape_sources

      - name: Detect changes (delta embeddings here)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          EMBED_DELTAS_ON_CHANGE: "true"
        run: python -m src.detect_changes

      - name: Warm up Modal vLLM (OpenAI-compatible)
        env:
          LLM_BASE_URL: ${{ secrets.LLM_BASE_URL }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          MODEL_TRIAGE: ${{ secrets.MODEL_TRIAGE }}
        run: |
          python - << 'PY'
          import os, time, requests

          base = os.environ["LLM_BASE_URL"].rstrip("/")
          key = os.environ["LLM_API_KEY"]
          model = os.environ.get("MODEL_TRIAGE", "mistral-small")
          headers = {"Authorization": f"Bearer {key}"}

          deadline = time.time() + 300  # 5 minutes
          attempt = 0

          def try_models():
            url = f"{base}/v1/models"
            r = requests.get(url, headers=headers, timeout=(10, 60))
            print("GET /v1/models:", r.status_code)
            return r.status_code < 400

          def try_chat():
            url = f"{base}/v1/chat/completions"
            payload = {
              "model": model,
              "messages": [{"role": "user", "content": "warmup"}],
              "max_tokens": 5,
              "temperature": 0
            }
            r = requests.post(url, headers=headers, json=payload, timeout=(10, 90))
            print("POST /v1/chat/completions:", r.status_code)
            return r.status_code < 400

          while time.time() < deadline:
            attempt += 1
            try:
              if try_models():
                print("Warmup success via /v1/models")
                raise SystemExit(0)
            except Exception as e:
              print(f"Attempt {attempt} /v1/models failed: {type(e).__name__}: {e}")

            try:
              if try_chat():
                print("Warmup success via chat")
                raise SystemExit(0)
            except Exception as e:
              print(f"Attempt {attempt} chat failed: {type(e).__name__}: {e}")

            sleep_s = min(10 + attempt * 2, 30)
            print(f"Sleeping {sleep_s}s before retry...")
            time.sleep(sleep_s)

          raise SystemExit("Warmup failed after 5 minutes")
          PY

      - name: Generate insights (Modal)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

          LLM_BASE_URL: ${{ secrets.LLM_BASE_URL }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

          MODEL_TRIAGE: ${{ secrets.MODEL_TRIAGE }}
          MODEL_ANALYZE: ${{ secrets.MODEL_ANALYZE }}

          RELEVANCE_THRESHOLD: "70"
          AGENT_NAME: "modal-digital-risk-agent"

          # Important for cold start / long first inference
          LLM_TIMEOUT_S: "180"
        run: python -m src.generate_insights_monstral